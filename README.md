Steps to use:

Open 3 terminals in container or server and run commands:

terminal 1 :
ollama serve

terimnal 2:
ollama run llama3.3:70b

terminal 3:
python llm.py
